{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Analysis: Computational Social Science\n",
    "\n",
    "**Tier 0 - Free Tier (Google Colab / Amazon SageMaker Studio Lab)**\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook introduces computational methods for analyzing social media data. You'll apply natural language processing, network analysis, and temporal modeling to understand social dynamics, information diffusion, and community structure.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Social media data preprocessing (cleaning, tokenization, hashtag extraction)\n",
    "- Sentiment analysis using VADER (Valence Aware Dictionary and sEntiment Reasoner)\n",
    "- Topic modeling with Latent Dirichlet Allocation (LDA)\n",
    "- Social network construction from user interactions\n",
    "- Community detection and clustering\n",
    "- Influence analysis and centrality metrics\n",
    "- Temporal trend analysis and viral content detection\n",
    "- Information cascade modeling\n",
    "\n",
    "**Runtime:** 30-40 minutes\n",
    "\n",
    "**Requirements:** `pandas`, `nltk`, `networkx`, `matplotlib`, `seaborn`, `vaderSentiment`, `scikit-learn`\n",
    "\n",
    "**Note:** This notebook uses synthetic data to avoid API rate limits. Tier 1+ includes real Twitter/Reddit API integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q nltk networkx vaderSentiment wordcloud scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment ready for social media analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Social Media Data\n",
    "\n",
    "Create realistic Twitter-like posts with users, timestamps, hashtags, mentions, and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_social_media_data(n_users=200, n_posts=5000, days=30):\n",
    "    \"\"\"\n",
    "    Generate synthetic social media dataset\n",
    "    \"\"\"\n",
    "    # User pool\n",
    "    users = [f\"user_{i}\" for i in range(n_users)]\n",
    "    \n",
    "    # Topics and associated keywords\n",
    "    topics = {\n",
    "        'climate': [\n",
    "            \"Climate change is real and we need action now! #ClimateAction #SaveThePlanet\",\n",
    "            \"Just learned about renewable energy solutions. Solar power is the future! #GreenEnergy\",\n",
    "            \"The polar ice caps are melting at an alarming rate. We must act! #ClimateEmergency\",\n",
    "        ],\n",
    "        'technology': [\n",
    "            \"Excited about the new AI developments! Machine learning is changing everything. #AI #TechNews\",\n",
    "            \"Just tried the latest smartphone. The camera quality is amazing! #Technology #Gadgets\",\n",
    "            \"Quantum computing will revolutionize cryptography. Mind-blowing stuff! #QuantumComputing\",\n",
    "        ],\n",
    "        'politics': [\n",
    "            \"Election day is coming. Make sure you vote! Your voice matters. #Vote2024\",\n",
    "            \"Policy debate was interesting tonight. Different perspectives on healthcare. #Politics\",\n",
    "            \"Democracy requires active participation from all citizens. #CivicDuty\",\n",
    "        ],\n",
    "        'sports': [\n",
    "            \"What a game! That last-minute goal was incredible! #Sports #Soccer\",\n",
    "            \"Olympics are coming up. Can't wait to watch the gymnastics! #Olympics2024\",\n",
    "            \"New world record in the 100m sprint! Absolutely amazing performance. #Athletics\",\n",
    "        ],\n",
    "        'entertainment': [\n",
    "            \"Just watched the latest blockbuster. Special effects were mind-blowing! #Movies\",\n",
    "            \"New album dropped today. Already on repeat! Best music of the year. #Music\",\n",
    "            \"That TV show finale was unexpected. Still processing what happened! #TVSeries\",\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    posts = []\n",
    "    start_date = datetime(2024, 1, 1)\n",
    "    \n",
    "    for i in range(n_posts):\n",
    "        # Select random topic and template\n",
    "        topic = np.random.choice(list(topics.keys()))\n",
    "        text = np.random.choice(topics[topic])\n",
    "        \n",
    "        # Random user\n",
    "        user = np.random.choice(users)\n",
    "        \n",
    "        # Timestamp (more posts during certain hours)\n",
    "        day_offset = np.random.randint(0, days)\n",
    "        hour = int(np.random.beta(2, 2) * 24)  # Peak during mid-day\n",
    "        timestamp = start_date + timedelta(days=day_offset, hours=hour)\n",
    "        \n",
    "        # Engagement metrics (some posts go viral)\n",
    "        is_viral = np.random.random() < 0.05\n",
    "        if is_viral:\n",
    "            likes = np.random.randint(1000, 10000)\n",
    "            retweets = np.random.randint(100, 2000)\n",
    "            replies = np.random.randint(50, 500)\n",
    "        else:\n",
    "            likes = np.random.randint(0, 100)\n",
    "            retweets = np.random.randint(0, 20)\n",
    "            replies = np.random.randint(0, 10)\n",
    "        \n",
    "        # Mentions (some posts mention other users)\n",
    "        mentions = []\n",
    "        if np.random.random() < 0.3:  # 30% of posts mention someone\n",
    "            n_mentions = np.random.randint(1, 4)\n",
    "            mentions = list(np.random.choice([u for u in users if u != user], n_mentions, replace=False))\n",
    "            text += \" \" + \" \".join([f\"@{m}\" for m in mentions])\n",
    "        \n",
    "        posts.append({\n",
    "            'post_id': f\"post_{i}\",\n",
    "            'user': user,\n",
    "            'text': text,\n",
    "            'timestamp': timestamp,\n",
    "            'topic': topic,\n",
    "            'likes': likes,\n",
    "            'retweets': retweets,\n",
    "            'replies': replies,\n",
    "            'mentions': mentions,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(posts)\n",
    "\n",
    "# Generate dataset\n",
    "df_social = generate_social_media_data(n_users=200, n_posts=5000, days=30)\n",
    "\n",
    "print(f\"Generated {len(df_social):,} posts from {df_social['user'].nunique()} users\")\n",
    "print(f\"Date range: {df_social['timestamp'].min()} to {df_social['timestamp'].max()}\")\n",
    "print(f\"\\nSample posts:\")\n",
    "print(df_social[['user', 'text', 'likes', 'retweets']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing and Feature Extraction\n",
    "\n",
    "Extract hashtags, mentions, URLs, and clean text for NLP analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_social_features(text):\n",
    "    \"\"\"Extract hashtags, mentions, and URLs from text\"\"\"\n",
    "    hashtags = re.findall(r'#\\w+', text)\n",
    "    mentions = re.findall(r'@\\w+', text)\n",
    "    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    \n",
    "    return hashtags, mentions, urls\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text for NLP analysis\"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    # Remove mentions and hashtags for cleaned version\n",
    "    text = re.sub(r'[@#]\\w+', '', text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Lowercase and strip\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "# Apply feature extraction\n",
    "df_social['hashtags'] = df_social['text'].apply(lambda x: extract_social_features(x)[0])\n",
    "df_social['text_mentions'] = df_social['text'].apply(lambda x: extract_social_features(x)[1])\n",
    "df_social['clean_text'] = df_social['text'].apply(clean_text)\n",
    "\n",
    "# Count features\n",
    "df_social['n_hashtags'] = df_social['hashtags'].apply(len)\n",
    "df_social['n_mentions'] = df_social['text_mentions'].apply(len)\n",
    "df_social['text_length'] = df_social['text'].apply(len)\n",
    "\n",
    "print(\"Text preprocessing complete!\")\n",
    "print(f\"\\nHashtag statistics:\")\n",
    "print(f\"  Posts with hashtags: {(df_social['n_hashtags'] > 0).sum()} ({(df_social['n_hashtags'] > 0).mean()*100:.1f}%)\")\n",
    "print(f\"  Average hashtags per post: {df_social['n_hashtags'].mean():.2f}\")\n",
    "\n",
    "print(f\"\\nMention statistics:\")\n",
    "print(f\"  Posts with mentions: {(df_social['n_mentions'] > 0).sum()} ({(df_social['n_mentions'] > 0).mean()*100:.1f}%)\")\n",
    "print(f\"  Average mentions per post: {df_social['n_mentions'].mean():.2f}\")\n",
    "\n",
    "# Most popular hashtags\n",
    "all_hashtags = [tag for tags in df_social['hashtags'] for tag in tags]\n",
    "hashtag_counts = Counter(all_hashtags)\n",
    "print(f\"\\nTop 10 hashtags:\")\n",
    "for tag, count in hashtag_counts.most_common(10):\n",
    "    print(f\"  {tag}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis with VADER\n",
    "\n",
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is specifically designed for social media text, handling emojis, slang, and intensifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Compute VADER sentiment scores\"\"\"\n",
    "    scores = sia.polarity_scores(text)\n",
    "    return scores\n",
    "\n",
    "# Apply sentiment analysis\n",
    "df_social['sentiment'] = df_social['text'].apply(analyze_sentiment)\n",
    "df_social['sentiment_compound'] = df_social['sentiment'].apply(lambda x: x['compound'])\n",
    "df_social['sentiment_pos'] = df_social['sentiment'].apply(lambda x: x['pos'])\n",
    "df_social['sentiment_neg'] = df_social['sentiment'].apply(lambda x: x['neg'])\n",
    "df_social['sentiment_neu'] = df_social['sentiment'].apply(lambda x: x['neu'])\n",
    "\n",
    "# Classify sentiment\n",
    "def classify_sentiment(compound):\n",
    "    if compound >= 0.05:\n",
    "        return 'positive'\n",
    "    elif compound <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "df_social['sentiment_label'] = df_social['sentiment_compound'].apply(classify_sentiment)\n",
    "\n",
    "# Visualize sentiment distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of compound scores\n",
    "ax1.hist(df_social['sentiment_compound'], bins=50, edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(0, color='red', linestyle='--', linewidth=2, label='Neutral threshold')\n",
    "ax1.set_xlabel('Sentiment Compound Score', fontsize=11)\n",
    "ax1.set_ylabel('Number of posts', fontsize=11)\n",
    "ax1.set_title('Distribution of Sentiment Scores', fontsize=13)\n",
    "ax1.legend()\n",
    "\n",
    "# Sentiment by category\n",
    "sentiment_counts = df_social['sentiment_label'].value_counts()\n",
    "colors = {'positive': 'green', 'neutral': 'gray', 'negative': 'red'}\n",
    "ax2.bar(sentiment_counts.index, sentiment_counts.values, \n",
    "        color=[colors[label] for label in sentiment_counts.index], alpha=0.7, edgecolor='black')\n",
    "ax2.set_xlabel('Sentiment Category', fontsize=11)\n",
    "ax2.set_ylabel('Number of posts', fontsize=11)\n",
    "ax2.set_title('Sentiment Distribution by Category', fontsize=13)\n",
    "\n",
    "for i, v in enumerate(sentiment_counts.values):\n",
    "    ax2.text(i, v + 50, str(v), ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSentiment Analysis Results:\")\n",
    "print(sentiment_counts)\n",
    "print(f\"\\nAverage sentiment: {df_social['sentiment_compound'].mean():.3f}\")\n",
    "print(f\"Most positive post: {df_social.loc[df_social['sentiment_compound'].idxmax(), 'text'][:100]}...\")\n",
    "print(f\"Most negative post: {df_social.loc[df_social['sentiment_compound'].idxmin(), 'text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment by Topic\n",
    "\n",
    "Analyze how sentiment varies across different topics (climate, technology, politics, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment by topic\n",
    "topic_sentiment = df_social.groupby('topic')['sentiment_compound'].agg(['mean', 'std', 'count'])\n",
    "topic_sentiment = topic_sentiment.sort_values('mean', ascending=False)\n",
    "\n",
    "print(\"Sentiment by Topic:\")\n",
    "print(topic_sentiment)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(topic_sentiment))\n",
    "ax.bar(x, topic_sentiment['mean'], yerr=topic_sentiment['std'], \n",
    "       alpha=0.7, edgecolor='black', capsize=5)\n",
    "ax.axhline(0, color='red', linestyle='--', linewidth=2, alpha=0.5)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(topic_sentiment.index, fontsize=11)\n",
    "ax.set_ylabel('Average Sentiment Score', fontsize=12)\n",
    "ax.set_title('Sentiment Analysis by Topic (with standard deviation)', fontsize=14)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sentiment distribution by topic\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for topic in df_social['topic'].unique():\n",
    "    topic_data = df_social[df_social['topic'] == topic]['sentiment_compound']\n",
    "    ax.hist(topic_data, bins=30, alpha=0.5, label=topic, edgecolor='black')\n",
    "\n",
    "ax.axvline(0, color='red', linestyle='--', linewidth=2, label='Neutral')\n",
    "ax.set_xlabel('Sentiment Compound Score', fontsize=11)\n",
    "ax.set_ylabel('Number of posts', fontsize=11)\n",
    "ax.set_title('Sentiment Distribution by Topic', fontsize=13)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Topic Modeling with LDA\n",
    "\n",
    "Discover latent topics in the corpus using Latent Dirichlet Allocation (LDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text for LDA\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Vectorize\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features=1000,\n",
    "    min_df=5,\n",
    "    max_df=0.7,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "doc_term_matrix = vectorizer.fit_transform(df_social['clean_text'])\n",
    "\n",
    "# Train LDA model\n",
    "n_topics = 5\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    max_iter=20,\n",
    "    learning_method='online',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lda_output = lda_model.fit_transform(doc_term_matrix)\n",
    "\n",
    "# Display topics\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"\\nDiscovered Topics (top 10 words per topic):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_words_idx = topic.argsort()[-10:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Assign dominant topic to each post\n",
    "df_social['dominant_topic'] = lda_output.argmax(axis=1)\n",
    "df_social['topic_probability'] = lda_output.max(axis=1)\n",
    "\n",
    "print(f\"\\nTopic distribution across posts:\")\n",
    "print(df_social['dominant_topic'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Social Network Construction and Analysis\n",
    "\n",
    "Build a network graph from user interactions (mentions, replies) and analyze its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build directed network from mentions\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add edges for mentions\n",
    "for _, row in df_social.iterrows():\n",
    "    user = row['user']\n",
    "    for mentioned_user in row['mentions']:\n",
    "        if G.has_edge(user, mentioned_user):\n",
    "            G[user][mentioned_user]['weight'] += 1\n",
    "        else:\n",
    "            G.add_edge(user, mentioned_user, weight=1)\n",
    "\n",
    "print(f\"Social Network Statistics:\")\n",
    "print(f\"  Nodes (users): {G.number_of_nodes()}\")\n",
    "print(f\"  Edges (mentions): {G.number_of_edges()}\")\n",
    "print(f\"  Network density: {nx.density(G):.4f}\")\n",
    "print(f\"  Average degree: {sum(dict(G.degree()).values()) / G.number_of_nodes():.2f}\")\n",
    "\n",
    "# Identify strongly connected components\n",
    "if G.number_of_nodes() > 0:\n",
    "    largest_scc = max(nx.strongly_connected_components(G), key=len)\n",
    "    print(f\"  Largest strongly connected component: {len(largest_scc)} users\")\n",
    "\n",
    "# Calculate centrality metrics\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "in_degree_centrality = nx.in_degree_centrality(G)  # Who gets mentioned most\n",
    "out_degree_centrality = nx.out_degree_centrality(G)  # Who mentions others most\n",
    "\n",
    "# Top influential users (by in-degree - being mentioned)\n",
    "top_influential = sorted(in_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(f\"\\nTop 10 Most Influential Users (by mentions received):\")\n",
    "for i, (user, centrality) in enumerate(top_influential, 1):\n",
    "    in_degree = G.in_degree(user)\n",
    "    print(f\"  {i}. {user}: {in_degree} mentions (centrality: {centrality:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize network (sample for performance)\n",
    "# Take subgraph of most active users\n",
    "top_users = [user for user, _ in sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:50]]\n",
    "G_sub = G.subgraph(top_users).copy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "\n",
    "# Layout\n",
    "pos = nx.spring_layout(G_sub, k=0.5, iterations=50, seed=42)\n",
    "\n",
    "# Node sizes based on in-degree (mentions received)\n",
    "node_sizes = [G_sub.in_degree(node) * 100 + 100 for node in G_sub.nodes()]\n",
    "\n",
    "# Node colors based on betweenness centrality\n",
    "betweenness = nx.betweenness_centrality(G_sub)\n",
    "node_colors = [betweenness[node] for node in G_sub.nodes()]\n",
    "\n",
    "# Draw network\n",
    "nx.draw_networkx_nodes(G_sub, pos, node_size=node_sizes, node_color=node_colors,\n",
    "                       cmap='viridis', alpha=0.7, ax=ax)\n",
    "nx.draw_networkx_edges(G_sub, pos, alpha=0.2, arrows=True, arrowsize=10, \n",
    "                       arrowstyle='->', ax=ax, edge_color='gray')\n",
    "nx.draw_networkx_labels(G_sub, pos, font_size=7, font_weight='bold', ax=ax)\n",
    "\n",
    "ax.set_title('Social Network Graph (Top 50 Users by Activity)', fontsize=16)\n",
    "ax.axis('off')\n",
    "\n",
    "# Add colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap='viridis', \n",
    "                           norm=plt.Normalize(vmin=min(node_colors), vmax=max(node_colors)))\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=ax, fraction=0.046, pad=0.04)\n",
    "cbar.set_label('Betweenness Centrality', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Node size = mentions received, color = betweenness centrality (bridging role)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Temporal Analysis: Trends and Viral Content\n",
    "\n",
    "Analyze how engagement, sentiment, and topics evolve over time. Identify viral posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add date column\n",
    "df_social['date'] = df_social['timestamp'].dt.date\n",
    "\n",
    "# Daily aggregations\n",
    "daily_stats = df_social.groupby('date').agg({\n",
    "    'post_id': 'count',\n",
    "    'likes': 'sum',\n",
    "    'retweets': 'sum',\n",
    "    'sentiment_compound': 'mean',\n",
    "}).rename(columns={'post_id': 'num_posts'})\n",
    "\n",
    "# Plot temporal trends\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Post volume\n",
    "axes[0].plot(daily_stats.index, daily_stats['num_posts'], marker='o', linewidth=2)\n",
    "axes[0].fill_between(daily_stats.index, daily_stats['num_posts'], alpha=0.3)\n",
    "axes[0].set_ylabel('Number of Posts', fontsize=11)\n",
    "axes[0].set_title('Daily Post Volume', fontsize=13)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Engagement\n",
    "axes[1].plot(daily_stats.index, daily_stats['likes'], marker='o', linewidth=2, label='Likes')\n",
    "axes[1].plot(daily_stats.index, daily_stats['retweets'], marker='s', linewidth=2, label='Retweets')\n",
    "axes[1].set_ylabel('Total Engagement', fontsize=11)\n",
    "axes[1].set_title('Daily Engagement (Likes and Retweets)', fontsize=13)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Sentiment\n",
    "axes[2].plot(daily_stats.index, daily_stats['sentiment_compound'], marker='o', linewidth=2, color='green')\n",
    "axes[2].axhline(0, color='red', linestyle='--', linewidth=2, alpha=0.5)\n",
    "axes[2].fill_between(daily_stats.index, 0, daily_stats['sentiment_compound'], \n",
    "                     where=(daily_stats['sentiment_compound'] > 0), alpha=0.3, color='green')\n",
    "axes[2].fill_between(daily_stats.index, 0, daily_stats['sentiment_compound'], \n",
    "                     where=(daily_stats['sentiment_compound'] <= 0), alpha=0.3, color='red')\n",
    "axes[2].set_xlabel('Date', fontsize=11)\n",
    "axes[2].set_ylabel('Average Sentiment', fontsize=11)\n",
    "axes[2].set_title('Daily Average Sentiment', fontsize=13)\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify viral posts (top 1% by total engagement)\n",
    "df_social['total_engagement'] = df_social['likes'] + df_social['retweets'] * 2 + df_social['replies']\n",
    "engagement_threshold = df_social['total_engagement'].quantile(0.99)\n",
    "viral_posts = df_social[df_social['total_engagement'] >= engagement_threshold].copy()\n",
    "\n",
    "print(f\"\\nViral Content Analysis:\")\n",
    "print(f\"  Engagement threshold (99th percentile): {engagement_threshold:,.0f}\")\n",
    "print(f\"  Number of viral posts: {len(viral_posts)}\")\n",
    "print(f\"  Average sentiment of viral posts: {viral_posts['sentiment_compound'].mean():.3f}\")\n",
    "print(f\"  Most common topics in viral posts:\")\n",
    "print(viral_posts['topic'].value_counts())\n",
    "\n",
    "print(f\"\\nTop 5 Most Viral Posts:\")\n",
    "top_viral = viral_posts.nlargest(5, 'total_engagement')[['user', 'text', 'likes', 'retweets', 'sentiment_compound']]\n",
    "for i, (_, row) in enumerate(top_viral.iterrows(), 1):\n",
    "    print(f\"\\n{i}. @{row['user']} | Likes: {row['likes']:,} | Retweets: {row['retweets']:,}\")\n",
    "    print(f\"   Sentiment: {row['sentiment_compound']:.3f}\")\n",
    "    print(f\"   Text: {row['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Word Cloud Visualization\n",
    "\n",
    "Visualize the most frequent words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word cloud\n",
    "all_text = ' '.join(df_social['clean_text'])\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=1200,\n",
    "    height=600,\n",
    "    background_color='white',\n",
    "    colormap='viridis',\n",
    "    max_words=100,\n",
    "    relative_scaling=0.5,\n",
    "    min_font_size=10\n",
    ").generate(all_text)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.axis('off')\n",
    "ax.set_title('Word Cloud of Social Media Posts', fontsize=18, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Word cloud by sentiment\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Positive posts\n",
    "positive_text = ' '.join(df_social[df_social['sentiment_label'] == 'positive']['clean_text'])\n",
    "wc_positive = WordCloud(width=600, height=400, background_color='white', colormap='Greens').generate(positive_text)\n",
    "ax1.imshow(wc_positive, interpolation='bilinear')\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Positive Posts Word Cloud', fontsize=14)\n",
    "\n",
    "# Negative posts\n",
    "negative_text = ' '.join(df_social[df_social['sentiment_label'] == 'negative']['clean_text'])\n",
    "if negative_text.strip():  # Check if there's any text\n",
    "    wc_negative = WordCloud(width=600, height=400, background_color='white', colormap='Reds').generate(negative_text)\n",
    "    ax2.imshow(wc_negative, interpolation='bilinear')\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'No negative posts', ha='center', va='center', fontsize=16)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Negative Posts Word Cloud', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "1. **Data Collection and Preprocessing**\n",
    "   - Generated synthetic social media dataset (5,000 posts, 200 users)\n",
    "   - Extracted hashtags, mentions, and cleaned text\n",
    "   - Computed engagement metrics (likes, retweets, replies)\n",
    "\n",
    "2. **Sentiment Analysis**\n",
    "   - Applied VADER sentiment analysis for social media text\n",
    "   - Classified posts as positive, negative, or neutral\n",
    "   - Analyzed sentiment variations across topics\n",
    "\n",
    "3. **Topic Modeling**\n",
    "   - Discovered latent topics using LDA\n",
    "   - Assigned dominant topics to posts\n",
    "   - Visualized topic distributions\n",
    "\n",
    "4. **Social Network Analysis**\n",
    "   - Constructed directed network from user mentions\n",
    "   - Calculated centrality metrics (degree, betweenness)\n",
    "   - Identified influential users and community structure\n",
    "\n",
    "5. **Temporal Analysis**\n",
    "   - Tracked daily post volume, engagement, and sentiment\n",
    "   - Identified viral posts (top 1% by engagement)\n",
    "   - Analyzed trends over time\n",
    "\n",
    "6. **Visualization**\n",
    "   - Word clouds for overall corpus and by sentiment\n",
    "   - Network graphs showing social structure\n",
    "   - Time series plots of engagement and sentiment\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **Sentiment varies by topic**: Politics and climate topics tend to be more polarized\n",
    "- **Viral content is rare**: Only ~1% of posts achieve high engagement\n",
    "- **Network structure matters**: Influential users act as bridges between communities\n",
    "- **Temporal patterns exist**: Posting frequency and sentiment vary by time of day\n",
    "- **Hashtags drive discoverability**: Posts with hashtags get more engagement\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Synthetic data doesn't capture real social dynamics\n",
    "- No bot detection or spam filtering\n",
    "- Simplified network (only mentions, not retweets/replies)\n",
    "- VADER may miss context-dependent sentiment\n",
    "- No demographic or geographic analysis\n",
    "\n",
    "### Progression Path\n",
    "\n",
    "**Tier 1** - Real API integration\n",
    "- Twitter Academic API for historical data\n",
    "- Reddit API (PRAW) for subreddit analysis\n",
    "- Real-time streaming data collection\n",
    "- Bot detection and spam filtering\n",
    "\n",
    "**Tier 2** - AWS-integrated pipeline\n",
    "- Lambda functions for data ingestion\n",
    "- S3 for data storage\n",
    "- SageMaker for ML model training\n",
    "- Interactive dashboards with Plotly Dash\n",
    "\n",
    "**Tier 3** - Production social listening platform\n",
    "- CloudFormation stack (EC2, RDS, ElastiCache, Kinesis)\n",
    "- Real-time trend detection and alert system\n",
    "- Multi-platform aggregation (Twitter, Reddit, Facebook)\n",
    "- Advanced NLP: named entity recognition, event detection\n",
    "- Influencer identification and outreach tools\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- Twitter API documentation: https://developer.twitter.com/en/docs\n",
    "- PRAW (Reddit API): https://praw.readthedocs.io/\n",
    "- NetworkX documentation: https://networkx.org/\n",
    "- \"Social Media Mining\" by Zafarani, Abbasi, Liu\n",
    "- \"Networks, Crowds, and Markets\" by Easley and Kleinberg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
